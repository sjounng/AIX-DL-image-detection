{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ìƒì„± ì´ë¯¸ì§€ íŒë³„ - ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë°ì´í„°ì…‹ì„ Train/Validation/Testë¡œ ë¶„í• í•˜ê³ , ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "2. ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "3. ë°ì´í„° ë¶„í•  (Train/Val/Test)\n",
    "4. ë¶„í•  ê²°ê³¼ í™•ì¸ ë° ì €ì¥\n",
    "5. ë°ì´í„° ë¶„í¬ ì‹œê°í™”\n",
    "6. Transform ì •ì˜ (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"Random Seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬\n",
    "PROJECT_ROOT = Path('.').absolute().parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results' / 'figures'\n",
    "\n",
    "FAKE_DIR = DATA_DIR / 'FAKE'\n",
    "REAL_DIR = DATA_DIR / 'REAL'\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
    "print(f\"ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}\")\n",
    "print(f\"ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}\")\n",
    "print(f\"\\nFAKE ë””ë ‰í† ë¦¬ ì¡´ì¬: {FAKE_DIR.exists()}\")\n",
    "print(f\"REAL ë””ë ‰í† ë¦¬ ì¡´ì¬: {REAL_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ë¶„í•  (Train/Val/Test)\n",
    "\n",
    "### ë¶„í•  ì „ëµ\n",
    "- **Train**: 70% (~28,000ì¥)\n",
    "- **Validation**: 15% (~6,000ì¥)\n",
    "- **Test**: 15% (~6,000ì¥)\n",
    "- **Stratified Split**: í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ (FAKE:REAL = 50:50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘\n",
    "print(\"ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...\")\n",
    "\n",
    "fake_images = list(FAKE_DIR.glob('*.jpg')) + list(FAKE_DIR.glob('*.png'))\n",
    "real_images = list(REAL_DIR.glob('*.jpg')) + list(REAL_DIR.glob('*.png'))\n",
    "\n",
    "print(f\"\\nFAKE ì´ë¯¸ì§€: {len(fake_images):,}ê°œ\")\n",
    "print(f\"REAL ì´ë¯¸ì§€: {len(real_images):,}ê°œ\")\n",
    "print(f\"ì „ì²´ ì´ë¯¸ì§€: {len(fake_images) + len(real_images):,}ê°œ\")\n",
    "\n",
    "# ê²½ë¡œì™€ ë ˆì´ë¸” ìƒì„±\n",
    "image_paths = fake_images + real_images\n",
    "labels = [0] * len(fake_images) + [1] * len(real_images)  # 0: FAKE, 1: REAL\n",
    "\n",
    "print(f\"\\në ˆì´ë¸” ë¶„í¬:\")\n",
    "print(f\"  FAKE (0): {labels.count(0):,}ê°œ\")\n",
    "print(f\"  REAL (1): {labels.count(1):,}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / (Val + Test) ë¶„í•  (70% / 30%)\n",
    "print(\"ë°ì´í„° ë¶„í•  ì¤‘...\\n\")\n",
    "\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths,\n",
    "    labels,\n",
    "    test_size=0.3,\n",
    "    stratify=labels,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"1ë‹¨ê³„ ì™„ë£Œ: Train / Temp ë¶„í• \")\n",
    "print(f\"  Train: {len(train_paths):,}ê°œ ({len(train_paths)/len(image_paths)*100:.1f}%)\")\n",
    "print(f\"  Temp:  {len(temp_paths):,}ê°œ ({len(temp_paths)/len(image_paths)*100:.1f}%)\")\n",
    "\n",
    "# Val / Test ë¶„í•  (ê°ê° 15%)\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # 30%ì˜ ì ˆë°˜ = 15%\n",
    "    stratify=temp_labels,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\n2ë‹¨ê³„ ì™„ë£Œ: Val / Test ë¶„í• \")\n",
    "print(f\"  Val:   {len(val_paths):,}ê°œ ({len(val_paths)/len(image_paths)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_paths):,}ê°œ ({len(test_paths)/len(image_paths)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë¶„í•  ê²°ê³¼ í™•ì¸ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„í•  ê²°ê³¼ ìƒì„¸ í™•ì¸\n",
    "print(\"=\" * 60)\n",
    "print(\"ë°ì´í„° ë¶„í•  ê²°ê³¼ (í´ë˜ìŠ¤ë³„)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def count_labels(label_list):\n",
    "    fake_count = label_list.count(0)\n",
    "    real_count = label_list.count(1)\n",
    "    return fake_count, real_count\n",
    "\n",
    "train_fake, train_real = count_labels(train_labels)\n",
    "val_fake, val_real = count_labels(val_labels)\n",
    "test_fake, test_real = count_labels(test_labels)\n",
    "\n",
    "print(f\"\\nTrain Set: {len(train_paths):,}ê°œ\")\n",
    "print(f\"  FAKE: {train_fake:,}ê°œ ({train_fake/len(train_paths)*100:.1f}%)\")\n",
    "print(f\"  REAL: {train_real:,}ê°œ ({train_real/len(train_paths)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nValidation Set: {len(val_paths):,}ê°œ\")\n",
    "print(f\"  FAKE: {val_fake:,}ê°œ ({val_fake/len(val_paths)*100:.1f}%)\")\n",
    "print(f\"  REAL: {val_real:,}ê°œ ({val_real/len(val_paths)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest Set: {len(test_paths):,}ê°œ\")\n",
    "print(f\"  FAKE: {test_fake:,}ê°œ ({test_fake/len(test_paths)*100:.1f}%)\")\n",
    "print(f\"  REAL: {test_real:,}ê°œ ({test_real/len(test_paths)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… í´ë˜ìŠ¤ ê· í˜•ì´ ëª¨ë“  ì„¸íŠ¸ì—ì„œ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame ìƒì„±\n",
    "print(\"\\nDataFrame ìƒì„± ë° CSV ì €ì¥ ì¤‘...\")\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'image_path': [str(p) for p in train_paths],\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'image_path': [str(p) for p in val_paths],\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'image_path': [str(p) for p in test_paths],\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# CSV ì €ì¥\n",
    "train_csv = OUTPUT_DIR / 'train.csv'\n",
    "val_csv = OUTPUT_DIR / 'val.csv'\n",
    "test_csv = OUTPUT_DIR / 'test.csv'\n",
    "\n",
    "train_df.to_csv(train_csv, index=False)\n",
    "val_df.to_csv(val_csv, index=False)\n",
    "test_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}\")\n",
    "print(f\"  - train.csv ({len(train_df):,}ê°œ)\")\n",
    "print(f\"  - val.csv ({len(val_df):,}ê°œ)\")\n",
    "print(f\"  - test.csv ({len(test_df):,}ê°œ)\")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\n=== train.csv ìƒ˜í”Œ ===\")\n",
    "print(train_df.head())\n",
    "print(f\"\\në ˆì´ë¸” ë¶„í¬:\\n{train_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° ë¶„í¬ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„í•  ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "datasets = ['Train', 'Validation', 'Test']\n",
    "data = [\n",
    "    (train_fake, train_real),\n",
    "    (val_fake, val_real),\n",
    "    (test_fake, test_real)\n",
    "]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "for ax, dataset, (fake, real) in zip(axes, datasets, data):\n",
    "    bars = ax.bar(['FAKE', 'REAL'], [fake, real], color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.set_title(f'{dataset} Set', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('ì´ë¯¸ì§€ ê°œìˆ˜', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Train/Validation/Test ë¶„í•  ê²°ê³¼', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ì €ì¥\n",
    "save_path = RESULTS_DIR / 'data_split_distribution.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë¶„í•  ë¹„ìœ¨ íŒŒì´ ì°¨íŠ¸\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "sizes = [len(train_paths), len(val_paths), len(test_paths)]\n",
    "labels = ['Train\\n(70%)', 'Validation\\n(15%)', 'Test\\n(15%)']\n",
    "colors = ['#FF9999', '#66B2FF', '#99FF99']\n",
    "explode = (0.05, 0, 0)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    "    textprops={'fontsize': 12, 'fontweight': 'bold'}\n",
    ")\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(14)\n",
    "\n",
    "ax.set_title('ë°ì´í„°ì…‹ ë¶„í•  ë¹„ìœ¨', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# ì €ì¥\n",
    "save_path = RESULTS_DIR / 'data_split_ratio.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transform ì •ì˜ (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸)\n",
    "\n",
    "### ì „ì²˜ë¦¬ ë‹¨ê³„\n",
    "1. **ë¦¬ì‚¬ì´ì§•**: 224x224 (ResNet, EfficientNet ë“±ì˜ í‘œì¤€ ì…ë ¥ í¬ê¸°)\n",
    "2. **ì •ê·œí™”**: ImageNet í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©\n",
    "3. **ë°ì´í„° ì¦ê°•** (Trainingë§Œ):\n",
    "   - Random Horizontal Flip\n",
    "   - Random Rotation\n",
    "   - Color Jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Training Transform (ë°ì´í„° ì¦ê°• í¬í•¨)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Validation & Test Transform (ì¦ê°• ì—†ìŒ)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"Transform ì •ì˜ ì™„ë£Œ!\")\n",
    "print(f\"\\nì´ë¯¸ì§€ í¬ê¸°: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"ì •ê·œí™”: ImageNet í‰ê· /í‘œì¤€í¸ì°¨\")\n",
    "print(f\"  Mean: {IMAGENET_MEAN}\")\n",
    "print(f\"  Std:  {IMAGENET_STD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ì ìš© ì˜ˆì‹œ ì‹œê°í™”\n",
    "sample_image_path = train_paths[0]\n",
    "original_image = Image.open(sample_image_path)\n",
    "\n",
    "# ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°\n",
    "print(f\"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {original_image.size}\")\n",
    "print(f\"ìƒ˜í”Œ ê²½ë¡œ: {sample_image_path.name}\")\n",
    "\n",
    "# Transform ì ìš© (ì¦ê°• ì—¬ëŸ¬ ë²ˆ)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('ë°ì´í„° ì¦ê°• ì˜ˆì‹œ (Training Transform)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# ì›ë³¸\n",
    "axes[0, 0].imshow(original_image)\n",
    "axes[0, 0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# ì¦ê°•ëœ ì´ë¯¸ì§€ë“¤\n",
    "for i, ax in enumerate(axes.flat[1:]):\n",
    "    # Transform ì ìš© (ToTensor ë° Normalize ì œì™¸)\n",
    "    augmented = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "    ])(original_image)\n",
    "    \n",
    "    ax.imshow(augmented)\n",
    "    ax.set_title(f'Augmented {i+1}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ì €ì¥\n",
    "save_path = RESULTS_DIR / 'data_augmentation_examples.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìš”ì•½\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "1. ë°ì´í„° ë¶„í•  (Train: 70%, Val: 15%, Test: 15%)\n",
    "2. Stratified splitìœ¼ë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
    "3. CSV íŒŒì¼ë¡œ ì €ì¥ (`data/processed/*.csv`)\n",
    "4. ë¶„í•  ê²°ê³¼ ì‹œê°í™”\n",
    "5. Transform ì •ì˜ (ë¦¬ì‚¬ì´ì§•, ì •ê·œí™”, ë°ì´í„° ì¦ê°•)\n",
    "\n",
    "### ğŸ“ ìƒì„±ëœ íŒŒì¼\n",
    "- `data/processed/train.csv` - Train set ê²½ë¡œ ëª©ë¡\n",
    "- `data/processed/val.csv` - Validation set ê²½ë¡œ ëª©ë¡\n",
    "- `data/processed/test.csv` - Test set ê²½ë¡œ ëª©ë¡\n",
    "- `results/figures/data_split_distribution.png` - ë¶„í•  ê²°ê³¼ ì‹œê°í™”\n",
    "- `results/figures/data_split_ratio.png` - ë¶„í•  ë¹„ìœ¨ íŒŒì´ ì°¨íŠ¸\n",
    "- `results/figures/data_augmentation_examples.png` - ë°ì´í„° ì¦ê°• ì˜ˆì‹œ\n",
    "\n",
    "### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„\n",
    "- **Phase 4**: PyTorch Dataset ë° DataLoader êµ¬í˜„\n",
    "- ì´ CSV íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë”©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
